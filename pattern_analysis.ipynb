{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib import pyplot\n",
    "from numpy import linalg\n",
    "from numpy import random\n",
    "from random import sample\n",
    "from scipy import signal \n",
    "from scipy import stats\n",
    "from shapely.geometry import Point, Polygon\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "\n",
    "import datetime\n",
    "import foursquare\n",
    "import geopandas\n",
    "import math\n",
    "import numpy\n",
    "import pandas\n",
    "import pickle\n",
    "import pymongo\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tessellation_uri = 'http://dati.comune.milano.it/dataset/806829b9-134b-40cf-b0e0-03e66c4f76d7/resource/3e355dd1-a8b8-483d-ac4d-03a62232ef38/download/ace_maggio_2011.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lantent Activity in Mobility Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_category(c):\n",
    "    if not c['categories']:\n",
    "        return [(c['id'],c['name'])]\n",
    "    else:\n",
    "        l = [(c['id'],c['name'])]\n",
    "        for ca in c['categories']:\n",
    "            l.extend(extract_category(ca))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_name = ['Arte e intrattenimento',\n",
    " 'Istituti superiori e università',\n",
    " 'Eventi',\n",
    " 'Cibi',\n",
    " 'Posti professionali e altri',\n",
    " 'Locali notturni',\n",
    " 'All\\'aperto & Ricreativi',\n",
    " 'Negozi e servizi',\n",
    " 'Viaggi e trasporti',\n",
    " 'Residenza']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern di mobilità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikemi_dataframe = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_lookup = {}\n",
    "fwd_lookup, bwd_lookup = {} , {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bwd():\n",
    "    global bwd_lookup\n",
    "    bwd_lookup = { v:k for k,v in fwd_lookup.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_filter = ['Data_prelievo'\n",
    "                  , 'Gio_settimana_prelievo'\n",
    "                  , 'Festivo_feriale_prelievo'\n",
    "                  , 'Stazione_prelievo'\n",
    "                  , 'Durata_sec'\n",
    "                  , 'Data_arrivo'\n",
    "                  , 'Gio_settimana_arrivo'\n",
    "                  , 'Festivo_feriale_arrivo'\n",
    "                  , 'Stazione_arrivo'\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unione dei dataframe\n",
    "# for yy in range(2015, 2019): \n",
    "#     for mm in range(1, 13):\n",
    "#         try:\n",
    "#             csv_uri = '/home/datasets/bikemi/{0}/{1:02d} {0}.csv'.format(yy,mm)\n",
    "#             next_df = pandas.read_csv(csv_uri,\n",
    "#                            lineterminator ='\\r',\n",
    "#                            encoding = 'iso8859_2',\n",
    "#                            sep = ';',\n",
    "#                            parse_dates = ['Data_prelievo','Data_arrivo'],\n",
    "#                            date_parser = lambda x: datetime.datetime.strptime(x,'%d/%m/%y %H:%M'),\n",
    "#                            decimal = '.'\n",
    "#                           )\n",
    "            \n",
    "#         except FileNotFoundError:\n",
    "#             pass\n",
    "#         else:\n",
    "#             for _, i in next_df.iterrows():\n",
    "                \n",
    "#                 # LOOKUP\n",
    "#                 a = i['Stazione_prelievo']\n",
    "#                 b = i['Stazione_arrivo']\n",
    "                \n",
    "#                 if a not in station_lookup:\n",
    "#                     station_lookup[a] = {i['Nome_stazione_prelievo']}\n",
    "#                 else:\n",
    "#                     station_lookup[a].add(i['Nome_stazione_prelievo'])\n",
    "                    \n",
    "#                 if b not in station_lookup:\n",
    "#                     station_lookup[b] = {i['Nome_stazione_arrivo']}\n",
    "#                 else:\n",
    "#                     station_lookup[b].add(i['Nome_stazione_arrivo'])\n",
    "                \n",
    "#             #SAVE\n",
    "#             bikemi_dataframe = pandas.concat([bikemi_dataframe, next_df[columns_filter]])\n",
    "\n",
    "\n",
    "# pickle.dump(bikemi_dataframe, open( \"archive/bikemi_dataframe.pkl\", \"wb\" ) )\n",
    "# pickle.dump(station_lookup, open( \"archive/station_lookup.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikemi_dataframe = pickle.load(open('archive/bikemi_dataframe.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pickle.load(open('archive/station_lookup.pkl', 'rb'))\n",
    "station_lookup = {k:list(v)[0] for k,v in tmp.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps = pickle.load(open('/home/datasets/bikemi/station_gps_location.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correzione stazioni "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stazioni inutilizzate\n",
    "for i in list(gps):\n",
    "    if i not in station_lookup:\n",
    "        del gps[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stazioni mancanti\n",
    "gps[2] = gps[402]\n",
    "gps[903] = gps[263]\n",
    "\n",
    "gps[90] = (45.484649, 9.195576)\n",
    "gps[92] = (45.465589, 9.186123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relookupping\n",
    "new_gps = []\n",
    "for i in range(max(gps)+1):\n",
    "    if i in gps:\n",
    "        fwd_lookup[i] = len(new_gps)\n",
    "        new_gps.append(gps[i])\n",
    "gps = new_gps\n",
    "update_bwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correzione POIs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4square\n",
    "CLIENT_ID = 'XDNRQIG15QP4PUZEMAGGVBPYHV1WXQMFXXAXZM410USDSTC3'\n",
    "CLIENT_SECRET = 'M55BW0UXPLUGACETI5ENMSF3WXGLDIENZAHE5VXPPVBYLSWO'\n",
    "client = foursquare.Foursquare(client_id=CLIENT_ID, client_secret=CLIENT_SECRET)\n",
    "categories = client.venues.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup macrocateg\n",
    "category_lookup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for macro_category in categories['categories']:\n",
    "    for id_cat, name_cat in extract_category(macro_category):\n",
    "        category_lookup[id_cat] = (macro_category['id'],macro_category['name'],name_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4square DB\n",
    "mongo_conn = pymongo.MongoClient(\"mongodb://marvin.nptlab.di.unimi.it\")\n",
    "six_db = mongo_conn['FoursquarePlacesMilan']\n",
    "places = six_db['places']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulizia POIS\n",
    "for i in places.find():\n",
    "        tmp = {\n",
    "            'name':i['name'],\n",
    "            'location':(\n",
    "                float(i['location']['lat']['$numberDouble']),\n",
    "                float(i['location']['lng']['$numberDouble'])\n",
    "            ),\n",
    "            'categories':{category_lookup[j['id']][0] for j in i['categories']}\n",
    "        }\n",
    "        pois.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tassellamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_aces = geopandas.read_file(tessellation_uri)\n",
    "vor = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poligoni delle regioni\n",
    "for _, r in raw_aces.iterrows():\n",
    "    lat = r.geometry.exterior.xy[1]\n",
    "    lng = r.geometry.exterior.xy[0]\n",
    "    vor[int(r.ACE)-1] = Polygon([(lat[i], lng[i]) for i in range(len(lat))]) # L'ARRAY ACE PARTE DA 1\n",
    "\n",
    "vor = numpy.array([vor[i] for i in range(len(vor))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relookupping\n",
    "for g in range(len(gps)):\n",
    "    for r in range(len(vor)):\n",
    "        if vor[r].contains(Point(gps[g])):\n",
    "            fwd_lookup[bwd_lookup[g]] = r\n",
    "            \n",
    "del bwd_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regioni non vuote\n",
    "unempty = numpy.array([True]*len(vor))\n",
    "for r in range(len(vor)):\n",
    "    unempty[r] = r in fwd_lookup.values()\n",
    "\n",
    "vor = vor[unempty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relookupping\n",
    "for k,v in dict(zip((i for i, kept in enumerate(unempty) if kept), range(len(vor)))).items():\n",
    "    for f,b in fwd_lookup.items():\n",
    "        if b == k:\n",
    "            fwd_lookup[f] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF e SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorie: solo quelle usate, senza lista completa\n",
    "categories = set()\n",
    "for i in pois:\n",
    "    for j in i['categories']: \n",
    "        categories.add(j)\n",
    "categories = list(categories)\n",
    "categories.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequenze assolute\n",
    "# abs_frequencies = numpy.zeros([len(vor), len(categories)])\n",
    "# for i in range(len(vor)):\n",
    "#     for p in pois:\n",
    "#         if vor[i].contains(Point(p['location'])):\n",
    "#             for k in p['categories']:\n",
    "#                 abs_frequencies[i,categories.index(k)] += 1\n",
    "\n",
    "# pickle.dump(abs_frequencies, open( \"archive/abs_frequencies.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_frequencies = pickle.load(open('archive/abs_frequencies.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_cat = [sum([abs_frequencies[i,j] for i in range(len(vor))]) for j in range(len(categories))] # n° poi per categoria\n",
    "dim_reg = [sum([abs_frequencies[i,j] for j in range(len(categories))]) for i in range(len(vor))] # n°  poi per regione\n",
    "num_reg = [len([i for i in range(len(vor)) if abs_frequencies[i,j]]) for j in range(len(categories))] # n° regioni per categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = numpy.zeros([len(vor),len(categories)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vor)):\n",
    "    for j in range(len(categories)):\n",
    "        tf = dim_cat[j]/dim_reg[i]\n",
    "        idf = len(vor)/num_reg[j]\n",
    "        tf_idf[i,j] = tf*math.log(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, _, _ = linalg.svd(tf_idf, full_matrices=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuboidi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(fwd_lookup, open( \"archive/fwd_lookup.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_gran = 10*60\n",
    "time_bin = [i for i in range(0,60*60*24*2,time_gran)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i_time_bin(t,f):\n",
    "    numpy.floor((60*(60*(24*(f)+t.hour)+t.minute)+t.second)/time_gran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arriving_cuboid = numpy.zeros([len(vor), len(vor), len(time_bin)])\n",
    "leaving_cuboid = numpy.zeros([len(vor), len(vor), len(time_bin)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _, r in bikemi_dataframe.iterrows():\n",
    "#     src = fwd_lookup[r['Stazione_prelievo']]\n",
    "#     dst = fwd_lookup[r['Stazione_arrivo']]\n",
    "    \n",
    "#     leav_time = i_time_bin(r['Data_prelievo'], r['Festivo_feriale_prelievo'])\n",
    "#     arr_time = i_time_bin(r['Data_arrivo'], r['Festivo_feriale_arrivo'])\n",
    "    \n",
    "#     leaving_cuboid[src, dst, leav_time] += 1\n",
    "#     arriving_cuboid[src, dst, arr_time] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arriving_cuboid = pickle.load(open('archive/arriving_cuboid.pkl', 'rb'))\n",
    "leaving_cuboid = pickle.load(open('archive/leaving_cuboid.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vor)):\n",
    "    a = [arriving_cuboid[r,i,t] for r in range(len(vor)) for t in range(len(time_bin))]\n",
    "    l = [leaving_cuboid[i,r,t] for r in range(len(vor)) for t in range(len(time_bin))]\n",
    "    m = a + l\n",
    "    v = u[i]+[1]\n",
    "    docs[i] = {'v':v, 'm':m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([docs[i]['m'][n] for i in range(len(vor)) for n in range(len(docs[i]['m']))])\n",
    "vocab = list(vocab)\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i in range(len(vor)):\n",
    "    tmp += docs[i]['m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=(16.5, 6.7))\n",
    "ax = fig.gca()\n",
    "eps = 10\n",
    "yell = 10\n",
    "y, x = numpy.histogram(tmp, numpy.arange(0, max(tmp)+eps, eps))\n",
    "ax.plot(x[yell:-1], y[yell:], marker='.',ms=2.5, linestyle='', color='maroon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = pyplot.figure(figsize=(16.5, 6.7))\n",
    "# ax = fig.gca()\n",
    "# ecdf = ECDF(tmp)\n",
    "# ax.plot(ecdf.x, ecdf.y, marker='.',ms=0.1, linestyle='--', color='coral')\n",
    "# ax.set_xlim(0, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy.mean(tmp), numpy.percentile(tmp, 50), numpy.std(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Probablilità secca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = numpy.array([[abs_frequencies[r,k]/sum(abs_frequencies[r]) for k in range(len(categories))] for r in range(len(vor))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(categories)):\n",
    "    tmp = probs[:,k]\n",
    "    print((numpy.mean(tmp), numpy.std(tmp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_probs = numpy.zeros([len(categories),len(categories)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, r in bikemi_dataframe.iterrows():\n",
    "#     src = fwd_lookup[r['Stazione_prelievo']]\n",
    "#     dst = fwd_lookup[r['Stazione_arrivo']]\n",
    "#     for ks in range(len(categories)):\n",
    "#         for kd in range(len(categories)):\n",
    "#             move_probs[ks,kd] += probs[src,ks]*probs[dst,kd]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Samplig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(vor, open('task_data/vor.pkl', 'wb'))\n",
    "# pickle.dump(vocab, open('task_data/vocab.pkl', 'wb'))\n",
    "# pickle.dump(categories, open('task_data/categories.pkl', 'wb'))\n",
    "# pickle.dump(docs, open('task_data/docs.pkl', 'wb'))\n",
    "# len(vor)*len(time_bin)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = numpy.zeros([len(categories),len(vocab)]) # wt # b := word topic distribution\n",
    "z = numpy.zeros([len(vor), len(vor)*len(time_bin)*2]) # ta # z := token topic assignment\n",
    "th = numpy.zeros([len(vor), len(categories)]) # dt # th := document topic distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al = 1.5 # varianza rnd_th\n",
    "eta = 1.5 # varianza rnd_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random assignment\n",
    "# for r in range(len(vor)):\n",
    "#     for n in range(len(docs[r]['m'])):\n",
    "#         w = vocab.index(docs[r]['m'][n])\n",
    "#         k = sample(range(len(categories)),1)[0]\n",
    "        \n",
    "#         b[k, w] += 1\n",
    "#         th[r,k] += 1\n",
    "#         z[r,n] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gibbs            \n",
    "# for iterat in range(1):\n",
    "#     # catena (vedi Ethen Liu)\n",
    "#     for r in range(len(vor)):\n",
    "#         for n in range(len(docs[r]['m'])):\n",
    "#             w = vocab.index(docs[r]['m'][n])\n",
    "#             th[r, z[r,n]] -= 1\n",
    "#             b[z[r,n], w] -= 1\n",
    "            \n",
    "#             probs = []\n",
    "#             for k in range(len(categories)):                \n",
    "#                 x1 = b[k,w] + eta\n",
    "#                 x2 = sum(b[k])+ eta*len(vocab)\n",
    "#                 x = x1/x2\n",
    "#                 y1 = th[r,k] + al\n",
    "#                 y2 = sum(th[r])+ al*len(categories)\n",
    "#                 y = y1/y2\n",
    "                \n",
    "#                 probs.append(x/y)\n",
    "                \n",
    "#             z[r,n] = random.choice(range(len(categories)), 1, False, p=(probs/sum(probs)))\n",
    "      \n",
    "#     # ricalcolo\n",
    "#     for r in range(len(vor)):\n",
    "#         for n in range(len(docs[r]['m'])):\n",
    "#             word_id = vocab.index(docs[r]['m'][n])\n",
    "#             k = z[r,n]\n",
    "            \n",
    "#             b[k, word_id] += 1\n",
    "#             th[r,k] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pickle.load(open('task_data/b.pkl', 'rb'))\n",
    "z = pickle.load(open('task_data/z.pkl', 'rb'))\n",
    "th = pickle.load(open('task_data/th.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = numpy.zeros([len(vor),len(categories)])\n",
    "e = numpy.zeros([len(categories),len(vocab)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(categories)):\n",
    "    for i in range(len(vocab)):\n",
    "        e[k,i] = (b[k,i] + eta)/(sum(b[k]) + len(vocab)*eta) # distribuzione categories sulle parole a priori\n",
    "    for r in range(len(vor)):\n",
    "        s[r,k] = (th[r,k] + al)/(sum(th[r]) + al*len(categories)) # distribuzione categoria sulle regioni a priori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dirichlet Multinomial Regression - Latent Dirichlet Allocation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = numpy.zeros([len(categories),len(vocab)]) # wt # b := word topic distribution\n",
    "z = numpy.zeros([len(vor), len(vor)*len(time_bin)*2], int) # ta # z := token topic assignment\n",
    "th = numpy.zeros([len(vor), len(categories)]) # dt # th := document topic distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRM spec\n",
    "l = numpy.zeros([len(categories),len(vor)])\n",
    "a = numpy.zeros([len(vor), len(categories)])\n",
    "\n",
    "# le nuove parole\n",
    "m = numpy.zeros([len(vor),len(vor)*len(time_bin)*2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    for k in range(len(categories)):\n",
    "        l[k] = random.normal(0.0, 1.0, len(vor)) # qui in vertà ci andrebbe var(s)\n",
    "        b[k] = random.dirichlet(e[k]) # b : distrubuzione categories sulle parole\n",
    "        \n",
    "    for r in range(len(vor)):\n",
    "        for k in range(len(categories)):\n",
    "            det = numpy.dot(docs[r]['v'], l[k])\n",
    "            a[r,k] = numpy.exp(det) # a : parametro per dirichlet\n",
    "        \n",
    "        th[r] = random.dirichlet(a[r]) # th : distrubuzione categories sulla regione\n",
    "\n",
    "        for n in range(len(docs[r]['m'])):\n",
    "            # print(r,n,end='\\r')\n",
    "            z[r,n] = random.multinomial(1, th[r]).tolist().index(1) # z : category i-regione n-parola = pattern\n",
    "            docs[r]['m'][n] = vocab[random.multinomial(1, b[z[r,n]]).tolist().index(1)]\n",
    "    \n",
    "    # ricalcolo\n",
    "    tth = numpy.zeros([len(vor),len(categories)])\n",
    "    bb = numpy.zeros([len(categories),len(vocab)])\n",
    "    for r in range(len(vor)):\n",
    "        for n in range(len(docs[r]['m'])):\n",
    "            w = vocab.index(docs[r]['m'][n])\n",
    "            k = z[r,n]\n",
    "            \n",
    "            bb[k,w] += 1\n",
    "            tth[r,k] += 1\n",
    "            \n",
    "    for k in range(len(categories)):\n",
    "        for i in range(len(vocab)):\n",
    "            e[k,i] = (bb[k,i] + eta)/(sum(bb[k]) + len(vocab)*eta)\n",
    "        for r in range(len(vor)):\n",
    "            s[r,k] = (tth[r,k] + al)/(sum(tth[r]) + al*len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = z.copy()\n",
    "t = [tt[i,j] for i in range(len(tt)) for j in range(len(tt[i]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=(16.5, 6.7))\n",
    "ax = fig.gca()\n",
    "y, x = numpy.histogram(t, numpy.arange(0, max(t)+2, 1))\n",
    "ax.plot(x[:-1], y, marker='.',ms=11.0, linestyle='', color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_name[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = z[fwd_lookup[334]] # duomo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=(16.5, 6.7))\n",
    "ax = fig.gca()\n",
    "y, x = numpy.histogram(t, numpy.arange(min(t), max(t)+2, 1))\n",
    "ax.plot(x[:-1], y, marker='.',ms=5.5, linestyle='', color='purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_a = 1 # Duomo\n",
    "reg_b = 3 # Cadorna\n",
    "arr_part_a = 0\n",
    "hh, mm = 12, 10\n",
    "\n",
    "categories_name[z[fwd_lookup[reg_a], int((60*hh+mm)/10)*reg_b+(arr_part_a*14400)]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python BikeMI",
   "language": "python",
   "name": "bikemi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
