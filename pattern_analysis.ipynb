{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from matplotlib import pyplot\n",
    "from numpy import linalg\n",
    "from numpy import random\n",
    "from random import sample\n",
    "from scipy import signal \n",
    "from scipy import stats\n",
    "from shapely.geometry import Point, Polygon\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.metrics import silhouette_samples\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "from collections import defaultdict\n",
    "\n",
    "import dmr\n",
    "import datetime\n",
    "import foursquare\n",
    "import geopandas\n",
    "import math\n",
    "import numpy\n",
    "import pandas\n",
    "import pickle\n",
    "import pymongo\n",
    "import time\n",
    "import seaborn\n",
    "import warnings\n",
    "import bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_leaf = (5.0, 3.0)\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaf(name):\n",
    "    fig.savefig('lab_plot/'+name+'.eps', dpi=150, format='eps', transparent=True, pad_inches=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO maglia fine ds98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tessellation_uri = 'http://dati.comune.milano.it/dataset/806829b9-134b-40cf-b0e0-03e66c4f76d7/resource/3e355dd1-a8b8-483d-ac4d-03a62232ef38/download/ace_maggio_2011.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lantent Activity in Mobility Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_name = [\"Cultura\",\n",
    " \"Istruzione\",\n",
    " \"Eventi\",\n",
    " \"Cibi\",\n",
    " \"Lavoro\",\n",
    " \"Night\",\n",
    " \"Ricreativi\",\n",
    " \"Negozi\",\n",
    " \"Trasporti\",\n",
    " \"Residenza\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories_name = [\"Arte e intrattenimento\",\n",
    "#  \"Istituti superiori e università\",\n",
    "#  \"Eventi\",\n",
    "#  \"Cibi\",\n",
    "#  \"Posti professionali e altri\",\n",
    "#  \"Locali notturni\",\n",
    "#  \"All'aperto & Ricreativi\",\n",
    "#  \"Negozi e servizi\",\n",
    "#  \"Viaggi e trasporti\",\n",
    "#  \"Residenza\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern di mobilità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikemi_dataframe = pandas.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_lookup = {}\n",
    "fwd_lookup, bwd_lookup = {} , {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_bwd():\n",
    "    global bwd_lookup\n",
    "    bwd_lookup = { v:k for k,v in fwd_lookup.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_filter = [ 'Cliente'\n",
    "                  , 'Data_prelievo'\n",
    "                  , 'Gio_settimana_prelievo'\n",
    "                  , 'Festivo_feriale_prelievo'\n",
    "                  , 'Stazione_prelievo'\n",
    "                  , 'Durata_sec'\n",
    "                  , 'Data_arrivo'\n",
    "                  , 'Gio_settimana_arrivo'\n",
    "                  , 'Festivo_feriale_arrivo'\n",
    "                  , 'Stazione_arrivo'\n",
    "                 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # unione dei dataframe\n",
    "# for yy in range(2015, 2019): \n",
    "#     for mm in range(1, 13):\n",
    "#         try:\n",
    "#             csv_uri = '/home/datasets/bikemi/{0}/{1:02d} {0}.csv'.format(yy,mm)\n",
    "#             next_df = pandas.read_csv(csv_uri,\n",
    "#                            lineterminator ='\\r',\n",
    "#                            encoding = 'iso8859_2',\n",
    "#                            sep = ';',\n",
    "#                            parse_dates = ['Data_prelievo','Data_arrivo'],\n",
    "#                            date_parser = lambda x: datetime.datetime.strptime(x,'%d/%m/%y %H:%M'),\n",
    "#                            decimal = '.'\n",
    "#                           )\n",
    "            \n",
    "#         except FileNotFoundError:\n",
    "#             pass\n",
    "#         else:\n",
    "#             print(yy,mm,end='\\r')\n",
    "#             for _, i in next_df.iterrows():\n",
    "                \n",
    "# #                 # LOOKUP\n",
    "# #                 a = i['Stazione_prelievo']\n",
    "# #                 b = i['Stazione_arrivo']\n",
    "                \n",
    "# #                 if a not in station_lookup:\n",
    "# #                     station_lookup[a] = {i['Nome_stazione_prelievo']}\n",
    "# #                 else:\n",
    "# #                     station_lookup[a].add(i['Nome_stazione_prelievo'])\n",
    "                    \n",
    "# #                 if b not in station_lookup:\n",
    "# #                     station_lookup[b] = {i['Nome_stazione_arrivo']}\n",
    "# #                 else:\n",
    "# #                     station_lookup[b].add(i['Nome_stazione_arrivo'])\n",
    "#                 pass\n",
    "                \n",
    "#             #SAVE\n",
    "#             bikemi_dataframe = pandas.concat([bikemi_dataframe, next_df[columns_filter]])\n",
    "\n",
    "\n",
    "# pickle.dump(bikemi_dataframe, open( \"archive/bikemi_dataframe.pkl\", \"wb\" ) )\n",
    "# # pickle.dump(station_lookup, open( \"archive/station_lookup.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bikemi_dataframe = pickle.load(open('archive/bikemi_dataframe.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pickle.load(open('archive/station_lookup.pkl', 'rb'))\n",
    "station_lookup = {k:list(v)[0] for k,v in tmp.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps = pickle.load(open('/home/datasets/bikemi/station_gps_location.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correzione stazioni "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stazioni inutilizzate\n",
    "for i in list(gps):\n",
    "    if i not in station_lookup:\n",
    "        del gps[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stazioni mancanti\n",
    "gps[2] = gps[402]\n",
    "gps[903] = gps[263]\n",
    "\n",
    "gps[90] = (45.484649, 9.195576)\n",
    "gps[92] = (45.465589, 9.186123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relookupping\n",
    "new_gps = []\n",
    "for i in range(max(gps)+1):\n",
    "    if i in gps:\n",
    "        fwd_lookup[i] = len(new_gps)\n",
    "        new_gps.append(gps[i])\n",
    "gps = new_gps\n",
    "update_bwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correzione POIs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4square\n",
    "CLIENT_ID = 'XDNRQIG15QP4PUZEMAGGVBPYHV1WXQMFXXAXZM410USDSTC3'\n",
    "CLIENT_SECRET = 'M55BW0UXPLUGACETI5ENMSF3WXGLDIENZAHE5VXPPVBYLSWO'\n",
    "client = foursquare.Foursquare(client_id=CLIENT_ID, client_secret=CLIENT_SECRET)\n",
    "categories = client.venues.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO rimozione delle 10 categorie madri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup macrocateg\n",
    "category_lookup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_category(c):\n",
    "    if not c['categories']:\n",
    "        return [(c['id'],c['name'])]\n",
    "    else:\n",
    "        l = [(c['id'],c['name'])]\n",
    "        for ca in c['categories']:\n",
    "            l.extend(extract_category(ca))\n",
    "        return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for macro_category in categories['categories']:\n",
    "    for id_cat, name_cat in extract_category(macro_category):\n",
    "        category_lookup[id_cat] = (macro_category['id'],macro_category['name'],name_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4square DB\n",
    "mongo_conn = pymongo.MongoClient(\"mongodb://marvin.nptlab.di.unimi.it\")\n",
    "six_db = mongo_conn['FoursquarePlacesMilan']\n",
    "places = six_db['places']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pois = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulizia POIS\n",
    "for i in places.find():\n",
    "        tmp = {\n",
    "            'name':i['name'],\n",
    "            'location':(\n",
    "                float(i['location']['lat']['$numberDouble']),\n",
    "                float(i['location']['lng']['$numberDouble'])\n",
    "            ),\n",
    "            'categories':{category_lookup[j['id']][0] for j in i['categories']}\n",
    "        }\n",
    "        pois.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tassellamento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_aces = geopandas.read_file(tessellation_uri)\n",
    "vor = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.variation([i for i in raw_aces.geometry.area])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, x = [pois[i]['location'][0] for i in range(len(pois))], [pois[i]['location'][1] for i in range(len(pois))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = raw_aces.plot(figsize=sq_leaf, color='dimgray', edgecolor='white', alpha=1.0)\n",
    "fig = plot.get_figure()\n",
    "ax = fig.gca()\n",
    "ax.plot(x, y, marker='.',ms=0.2, linestyle='', color='orangered')\n",
    "ax.set_xlim(9.06, 9.28)\n",
    "ax.set_ylim(45.39, 45.54)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf('pois-galaxy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poligoni delle regioni\n",
    "for _, r in raw_aces.iterrows():\n",
    "    lat = r.geometry.exterior.xy[1]\n",
    "    lng = r.geometry.exterior.xy[0]\n",
    "    vor[int(r.ACE)-1] = Polygon([(lat[i], lng[i]) for i in range(len(lat))]) # L'ARRAY ACE PARTE DA 1\n",
    "\n",
    "vor = numpy.array([vor[i] for i in range(len(vor))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relookupping\n",
    "for g in range(len(gps)):\n",
    "    for r in range(len(vor)):\n",
    "        if vor[r].contains(Point(gps[g])):\n",
    "            fwd_lookup[bwd_lookup[g]] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [float(i+1) for i in list(set(fwd_lookup.values()))]\n",
    "qqq = raw_aces[raw_aces.ACE.isin(tmp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = qqq.plot(figsize=sq_leaf, color='dimgray', edgecolor='white', alpha=1.0)\n",
    "fig = plot.get_figure()\n",
    "ax = fig.gca()\n",
    "ax.set_xlim(9.085, 9.24)\n",
    "ax.set_ylim(45.42, 45.54)\n",
    "x, y = [gps[i][1] for i in range(len(gps))], [gps[i][0] for i in range(len(gps))]\n",
    "ax.plot(x, y, marker='.',ms=3.75, linestyle='', color='gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf('bikemi-galaxy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regioni non vuote\n",
    "unempty = numpy.array([True]*len(vor))\n",
    "for r in range(len(vor)):\n",
    "    unempty[r] = r in fwd_lookup.values()\n",
    "\n",
    "vor = vor[unempty]\n",
    "# vor = vor[list(set(fwd_lookup.values()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relookupping\n",
    "for k,v in dict(zip((i for i, kept in enumerate(unempty) if kept), range(len(vor)))).items():\n",
    "    for f,b in fwd_lookup.items():\n",
    "        if b == k:\n",
    "            fwd_lookup[f] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF e SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorie: solo quelle usate, senza lista completa\n",
    "categories = set()\n",
    "for i in pois:\n",
    "    for j in i['categories']: \n",
    "        categories.add(j)\n",
    "categories = list(categories)\n",
    "categories.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequenze assolute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs_frequencies = numpy.zeros([len(vor), len(categories)])\n",
    "# for r in range(len(vor)):\n",
    "#     for p in pois:\n",
    "#         if vor[r].contains(Point(p['location'])):\n",
    "#             for k in p['categories']:\n",
    "#                 abs_frequencies[r, categories.index(k)] += 1\n",
    "# pickle.dump(abs_frequencies, open( \"archive/abs_frequencies.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abs_frequencies = numpy.zeros([len(vor), len(categories)])\n",
    "# for r in range(len(vor)):\n",
    "#     for p in pois:\n",
    "#         if vor[r].contains(Point(p['location'])):\n",
    "#             for k in p['categories']:\n",
    "#                 abs_frequencies[r, random.choice(len(categories))] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_frequencies = pickle.load(open('archive/abs_frequencies.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rimozione categoria brutta\n",
    "abs_frequencies = numpy.delete(abs_frequencies, 2, 1)\n",
    "del categories[2]\n",
    "del categories_name[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_cat = [sum(abs_frequencies[:,k]) for k in range(len(categories))] # n° poi per categoria\n",
    "dim_reg = [sum(abs_frequencies[r,:]) for r in range(len(vor))] # n°  poi per regione\n",
    "num_reg = [len(abs_frequencies[:,k][abs_frequencies[:,k] != 0]) for k in range(len(categories))] # n° regioni per categoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=sq_leaf)\n",
    "ax = fig.gca()\n",
    "ax.barh(categories_name, dim_cat, color='darkgreen', log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf('category-hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = numpy.zeros([len(vor),len(categories)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vor)):\n",
    "    for j in range(len(categories)):\n",
    "        tf = dim_cat[j]/dim_reg[i]\n",
    "        idf = len(vor)/num_reg[j]\n",
    "        tf_idf[i,j] = tf*1 # math.log(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, v = linalg.svd(tf_idf, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO low-rank di s, e U, V ridotte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative features\n",
    "fig = pyplot.figure(figsize=sq_leaf)\n",
    "ax = fig.gca()\n",
    "ax = seaborn.heatmap(tf_idf, linewidth=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cuboidi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_gran = 60*12\n",
    "time_max = 60*60*18*2\n",
    "time_bin = [i for i in range(0,time_max,time_gran)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i_time_bin(t,f):\n",
    "    return int((60*(60*(18*(f)+((t.hour+17)%24))+t.minute)+t.second)/time_gran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arriving_cuboid = numpy.zeros([len(vor), len(vor), len(time_bin)])\n",
    "leaving_cuboid = numpy.zeros([len(vor), len(vor), len(time_bin)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_users = numpy.array([[[set()]*len(time_bin)]*len(vor)]*len(vor))\n",
    "leav_users = numpy.array([[[set()]*len(time_bin)]*len(vor)]*len(vor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, r in bikemi_dataframe.iterrows():\n",
    "#     src = fwd_lookup[r['Stazione_prelievo']]\n",
    "#     dst = fwd_lookup[r['Stazione_arrivo']]\n",
    "#     intr = range(1,7)\n",
    "    \n",
    "#     if(r['Data_arrivo'].hour not in intr and r['Data_prelievo'].hour not in intr):\n",
    "        \n",
    "#         arr_time = i_time_bin(r['Data_arrivo'], r['Festivo_feriale_arrivo'])\n",
    "#         leav_time = i_time_bin(r['Data_prelievo'], r['Festivo_feriale_prelievo'])\n",
    "#         usr = r['Cliente']\n",
    "        \n",
    "#         if usr not in arr_users[src, dst, arr_time]:\n",
    "#             arriving_cuboid[src, dst, arr_time] += 1\n",
    "#             arr_users[src, dst, arr_time].add(usr)\n",
    "#         if usr not in leav_users[src, dst, leav_time]:\n",
    "#             leaving_cuboid[src, dst, leav_time] += 1\n",
    "#             leav_users[src, dst, arr_time].add(usr)\n",
    "        \n",
    "# pickle.dump(arriving_cuboid, open( \"archive/arriving_cuboid.pkl\", \"wb\"))\n",
    "# pickle.dump(leaving_cuboid, open( \"archive/leaving_cuboid.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arriving_cuboid = pickle.load(open('archive/arriving_cuboid.pkl', 'rb'))\n",
    "leaving_cuboid = pickle.load(open('archive/leaving_cuboid.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(len(vor)):\n",
    "    a = arriving_cuboid[:,r,:].ravel()\n",
    "    l = leaving_cuboid[r,:,:].ravel()\n",
    "    m = numpy.concatenate((a, l), axis=None)\n",
    "    v = u[r]+[1]\n",
    "    docs[r] = {'v':v, 'm':m}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(numpy.array([docs[r]['m'] for r in range(len(vor))]).ravel())\n",
    "vocab = list(vocab)\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for r in range(len(vor)):\n",
    "    tmp += docs[r]['m'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=sq_leaf)\n",
    "ax = fig.gca()\n",
    "eps = 1\n",
    "yell = 1\n",
    "y, x = numpy.histogram(tmp, numpy.arange(0, max(tmp)+eps, eps))\n",
    "ax.plot(x[yell:-1], y[yell:], marker='.',ms=5.5, linestyle='', color='maroon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.mean(tmp), numpy.percentile(tmp, 50), numpy.std(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Probablilità congiunta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = numpy.zeros([len(vor), len(categories)])\n",
    "for r in range(len(abs_frequencies)):\n",
    "    probs[r] = abs_frequencies[r]/numpy.sum(abs_frequencies[r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(probs, open(\"means_k/probs/probs.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_probs = numpy.zeros([len(vor),len(vor)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users = numpy.array([[set()]*len(vor)]*len(vor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, r in bikemi_dataframe.iterrows():\n",
    "#     src = fwd_lookup[r['Stazione_prelievo']]\n",
    "#     dst = fwd_lookup[r['Stazione_arrivo']]\n",
    "#     usr = r['Cliente']\n",
    "\n",
    "#     if usr not in users[src, dst]:\n",
    "#         move_probs[src, dst] += 1\n",
    "#         users[src, dst].add(usr)\n",
    "            \n",
    "# pickle.dump(move_probs, open( \"archive/move_probs.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "move_probs = pickle.load(open( \"archive/move_probs.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conj_probs = numpy.zeros([len(categories),len(categories)])\n",
    "tmp = numpy.sum(move_probs)\n",
    "for ks in range(len(categories)):\n",
    "    for kd in range(len(categories)):\n",
    "        for rs in range(len(vor)):\n",
    "            for rd in range(len(vor)):\n",
    "                conj_probs[ks,kd] += probs[rs,ks]*probs[rd,kd]*move_probs[rs,rd]/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_probs = numpy.zeros([len(categories),len(categories)])\n",
    "for ks in range(len(categories)):\n",
    "    tmp = numpy.sum(conj_probs[ks,:])\n",
    "    for kd in range(len(categories)):\n",
    "        cond_probs[ks,kd] = conj_probs[ks,kd]/tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Category ditribution over each region\n",
    "\n",
    "fig = pyplot.figure(figsize=sq_leaf)\n",
    "ax = fig.gca()\n",
    "# ax = seaborn.heatmap(abs_frequencies, linewidth=0.5)\n",
    "ax = seaborn.heatmap(probs, linewidth=0.05)\n",
    "# ax = seaborn.heatmap(move_probs, linewidth=0.5)\n",
    "# ax = seaborn.heatmap(conj_probs, linewidth=0.5)\n",
    "# ax = seaborn.heatmap(cond_probs, linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "{categories_name.index(i):i for i in categories_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diriclet Multinomial Regression di Kato "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "K = len(categories)\n",
    "sigma = 0.01\n",
    "beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, coeff_vecs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(len(docs)):\n",
    "    corpus.append(numpy.array(docs[r]['m']))\n",
    "    coeff_vecs.append(numpy.array(docs[r]['v']))\n",
    "corpus, coeff_vecs = numpy.array(corpus), numpy.array(coeff_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca = dmr.Vocabulary()\n",
    "doku = voca.read_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = dmr.DMR(K, sigma, beta, doku, coeff_vecs, voca.size())\n",
    "mdl.learning(iteration=50, voca=voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th, b, z = mdl.topicdist(), mdl.worddist(), mdl.z_m_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Category Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=sq_leaf)\n",
    "ax = fig.gca()\n",
    "ax.bar(range(len(mdl.n_z)), mdl.n_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{categories_name.index(i):i for i in categories_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in categories_name:\n",
    "    qqq[i] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(categories)):\n",
    "    for j, r in qqq.iterrows():\n",
    "        qqq.loc[j, categories_name[i]] = th[sum(unempty[:int(r.ACE)])-1,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in categories_name:\n",
    "    plot = qqq.plot(figsize=sq_leaf, edgecolor='white', column=w, legend=True);\n",
    "    fig = plot.get_figure()\n",
    "    ax = fig.gca()\n",
    "    ax.set_title(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing th\n",
    "for i in range(len(th)):\n",
    "    th[i] /= numpy.sum(th[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def means(X, n, file_name=None):\n",
    "    # Calculous\n",
    "    labels = KMeans(\n",
    "        n_clusters=n,\n",
    "        init='random',\n",
    "        n_init=240, \n",
    "        max_iter=10000,\n",
    "        precompute_distances=True,\n",
    "        random_state=42,\n",
    "        n_jobs=24,\n",
    "        algorithm='elkan'\n",
    "    ).fit_predict(X)\n",
    "    values = silhouette_samples(X, labels)\n",
    "    \n",
    "    if file_name is not None:\n",
    "        # Imaging\n",
    "        fig, ax1 = plt.subplots(1,1)\n",
    "        fig.set_size_inches(11.7, 16.5)\n",
    "        ax1.set_xlim([-0.5, 1.0])\n",
    "        ax1.set_ylim([0, len(X) + (n + 1) * 10])\n",
    "        y_lower = 10\n",
    "        for i in range(n):\n",
    "            i_values = values[labels == i]\n",
    "            i_values.sort()\n",
    "            i_size = i_values.shape[0]\n",
    "            y_upper = y_lower + i_size\n",
    "            color = cm.nipy_spectral(float(i) / n)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),0, i_values,facecolor=color, edgecolor=color, alpha=0.7)\n",
    "            ax1.text(-0.05, y_lower + 0.5 * i_size, str(i))\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "        \n",
    "        ax1.set_title(\"Cluster silhouette\")\n",
    "        ax1.set_xlabel(\"Silhouette coefficient : \"+str(avg[n]))\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "        ax1.axvline(x=avg[n], color=\"red\", linestyle=\"--\")\n",
    "        ax1.set_yticks([])\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "        plt.suptitle((\"Silhouette analysis: KMeans on %d\" % n), fontsize=14, fontweight='bold')\n",
    "        fig.savefig('means_k/'+file_name+'.png', dpi=150)\n",
    "    \n",
    "    return (labels, silhouette_score(X, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_clust, pre_clust = {}, {}\n",
    "for i in range(8,25):\n",
    "    post_clust[i] = means(th, i)\n",
    "for i in range(8,25):\n",
    "    pre_clust[i] = means(probs, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ya = [post_clust[i][1] for i in post_clust]\n",
    "yb = [pre_clust[i][1] for i in pre_clust]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering score pre-lda vs post-lda\n",
    "fig = pyplot.figure(figsize=sq_leaf)\n",
    "ax = fig.gca()\n",
    "ax.plot(list(post_clust.keys()), ya, marker='o',ms=5.5, linestyle='-', color='orange')\n",
    "ax.plot(list(pre_clust.keys()), yb, marker='o',ms=5.5, linestyle='-', color='lightblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion slice distro\n",
    "fig = pyplot.figure(figsize=sq_leaf)\n",
    "ax = fig.gca()\n",
    "eps = 0.005\n",
    "yell = 0\n",
    "y, x = numpy.histogram(th.ravel(), numpy.arange(0, 1, eps))\n",
    "ax.plot(x[yell:-1], y[yell:], marker='.',ms=5.5, linestyle='', color='maroon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "be_cl, af_cl = [i[0] for i in pre_clust.values()], [i[0] for i in post_clust.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmi = numpy.zeros((len(be_cl), len(af_cl)))\n",
    "for i in range(len(be_cl)):\n",
    "    for j in range(len(af_cl)):\n",
    "        nmi[i,j] = normalized_mutual_info_score(be_cl[i],af_cl[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=sq_leaf)\n",
    "ax = fig.gca()\n",
    "ax = seaborn.heatmap(nmi, linewidth=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### improprety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proper_index(a, b):\n",
    "    metrics = {}\n",
    "    for label in set(a):\n",
    "        metrics[label] = [0.0, 0.0]\n",
    "        inner_labels = set(b[numpy.where(a == label)])\n",
    "        for proper_label in inner_labels:\n",
    "            t_set = set(numpy.where(b == proper_label)[0])\n",
    "            a_set = set(numpy.where(a == label)[0])\n",
    "            metrics[label][0] += len(t_set - a_set)\n",
    "            metrics[label][1] += len(t_set)\n",
    "        metrics[label] = metrics[label][0] / metrics[label][1]\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = numpy.zeros((len(be_cl), len(af_cl)))\n",
    "for i in range(len(be_cl)):\n",
    "    for j in range(len(af_cl)):\n",
    "        cpi = proper_index(be_cl[i],af_cl[j])\n",
    "        pi[i,j] = numpy.mean(list(cpi.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=sq_leaf)\n",
    "ax = fig.gca()\n",
    "ax = seaborn.heatmap(pi, linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp=[]\n",
    "# for a in pre_clust[0].values():\n",
    "#     for b in post_clust[0].values():\n",
    "#         tmp.append(list(proper_index(a, b).values()))\n",
    "# fig = pyplot.figure(figsize=(32,5))\n",
    "# ax = fig.gca()\n",
    "# ax.boxplot(tmp) # pre k = varianza , post k= propritetà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaf(fig,'improprity-distro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample algorthm try\n",
    "# n = 4\n",
    "# x = numpy.array([i for i in range(n) for j in range(n)])\n",
    "# y = numpy.array([i for i in range(int(n/2)) for j in range(n*2)])\n",
    "# proper_index(y,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bands, rl, cl = 5, len(time_bin)//2, len(vor)*2*2\n",
    "bl = rl//bands\n",
    "for r in range(len(docs)):\n",
    "    tmp = []\n",
    "    for b in range(bands):\n",
    "        bnd = numpy.array([numpy.arange(bl*b+k,bl*b+k+bl) for k in range(0, cl*rl, rl)]).ravel()\n",
    "        tmp.append(docs[r]['m'][bnd])\n",
    "    docs[r]['m'] = numpy.array(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda = {}\n",
    "# for i in range(bands):\n",
    "#     corpus, coeff_vecs = [], []\n",
    "    \n",
    "#     for r in range(len(docs)):\n",
    "#         corpus.append(numpy.array(docs[r]['m'][i]))\n",
    "#         coeff_vecs.append(numpy.array(docs[r]['v']))\n",
    "#     corpus, coeff_vecs = numpy.array(corpus), numpy.array(coeff_vecs)\n",
    "    \n",
    "#     # learning\n",
    "#     voca = dmr.Vocabulary()\n",
    "#     doku = voca.read_corpus(corpus)\n",
    "#     lda[i] = dmr.DMR(K, sigma, beta, doku, coeff_vecs, voca.size())\n",
    "#     lda[i].learning(iteration=800, voca=voca)\n",
    "    \n",
    "# pickle.dump(lda, open( \"ultra_lda.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = pickle.load(open(\"ultra_lda.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th, b, z = {},{},{}\n",
    "for i in range(bands):\n",
    "    th[i] = lda[i].topicdist() # topic probability of each document\n",
    "    b[i] = lda[i].worddist() # word probability of each topic\n",
    "    z[i] = lda[i].z_m_n # maybe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(bands):\n",
    "    fig = pyplot.figure(figsize=sq_leaf)\n",
    "    ax = fig.gca()\n",
    "    ax = seaborn.heatmap(th[i], linewidth=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sqatted cuboids\n",
    "migr = numpy.array([None]*bands)\n",
    "dim = numpy.shape(arriving_cuboid)[2]//bands\n",
    "for i in range(bands):\n",
    "    migr[i] = numpy.sum(arriving_cuboid[:,:,dim*i:dim*(i+1)], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_indexes(arr, n):\n",
    "    idx = bottleneck.argpartition(arr, arr.size-n, axis=None)[-n:]\n",
    "    width = arr.shape[1]\n",
    "    return [divmod(i, width) for i in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in migr:\n",
    "#     for j in top_n_indexes(i,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DRM di Zheng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zheng_z = pickle.load(open(\"zheng_dmr/z.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = zheng_z\n",
    "t = [ tmp[j][i] for j in range(len(tmp)) for i in range(len(tmp[j])) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=sq_leaf)\n",
    "ax = fig.gca()\n",
    "y, x = numpy.histogram(t, numpy.arange(0, max(t)+2, 1))\n",
    "ax.plot(x[:-1], y, marker='.',ms=11.0, linestyle='', color='purple')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = {}\n",
    "for i, r in bikemi_dataframe.iterrows():\n",
    "    t = r.Data_prelievo.date()\n",
    "    if t not in qt:\n",
    "        qt[t] = set()\n",
    "    qt[t].add(r['Cliente'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [len(i) for i in qt.values()]\n",
    "fig = pyplot.figure(figsize=sq_leaf)\n",
    "ax = fig.gca()\n",
    "eps = 400\n",
    "yell = 0\n",
    "y, x = numpy.histogram(tmp, numpy.arange(0, max(tmp)+eps, eps))\n",
    "ax.plot(x[yell:-1], y[yell:], marker='.',ms=5.5, linestyle='-', color='maroon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pyplot.figure(figsize=sq_leaf)\n",
    "ax = fig.gca()\n",
    "from scipy import stats\n",
    "stats.probplot(tmp, dist=\"norm\", plot=ax)\n",
    "# pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.mean(tmp), numpy.median(tmp), numpy.percentile(tmp, 25), numpy.percentile(tmp, 75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python BikeMI",
   "language": "python",
   "name": "bikemi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
